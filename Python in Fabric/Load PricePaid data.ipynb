{"cells":[{"cell_type":"markdown","source":["# Load a CSV file into a Spark dataframe and then save the data as tables in our lakehouse.\n"," \n","The CSV file contains property sales in England since 1995.  The original data is from HM Land Registry in the UK.\n","For more details - see https://www.gov.uk/government/collections/price-paid-data.  \n","The data contains over 28 million rows - each row represents the sale of a property in England with details about the date, price paid, the property type (whether Flat, Terrace etc) and geographic data, most importantly the postcode.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5e31c096-8622-4484-9660-c5ae7b6cf2ff"},{"cell_type":"markdown","source":["Build the string of the full path to access the external source data PricePaid file. "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"168ad8f8-3ae4-466e-8673-2ec30e60fe29"},{"cell_type":"code","source":["# Details of the external CSV file\n","storage_account = \"zomalextrainingstorage\"\n","container = \"datasets\"\n","folder = \"price-paid\"\n","filename = \"pp-complete.csv\"\n","#sas_token = r\"\" # Blank since the datasets container is anonymous access\n","\n","# Create the full file path to the CSV file using the WASB (blob) access protocol\n","wasbs_path = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/{folder}/{filename}\"\n","wasbs_path"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"c535fe02-1ea9-4fa3-a399-680f91e5cfa1","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2023-09-29T13:18:12.5857703Z","session_start_time":null,"execution_start_time":"2023-09-29T13:18:13.0741157Z","execution_finish_time":"2023-09-29T13:18:13.3913507Z","spark_jobs":{"numbers":{"UNKNOWN":0,"RUNNING":0,"SUCCEEDED":0,"FAILED":0},"jobs":[],"limit":20,"rule":"ALL_DESC"},"parent_msg_id":"b16b1e6d-8bac-4a35-9d61-620bbbc308c2"},"text/plain":"StatementMeta(, c535fe02-1ea9-4fa3-a399-680f91e5cfa1, 4, Finished, Available)"},"metadata":{}},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"'wasbs://datasets@zomalextrainingstorage.blob.core.windows.net/price-paid/pp-complete.csv'"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"07b07301-42eb-499c-aacd-5b55e7748a27"},{"cell_type":"markdown","source":["Load the data in the CSV file into a Spark dataframe.  Show the first two rows."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ab8a8b8b-7e71-41ad-88a4-8821cfa98244"},{"cell_type":"code","source":["df = spark.read.option(\"header\",\"false\").csv(wasbs_path)\n","df.limit(2).show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"60fcc0c6-1001-42ef-8340-664847228133"},{"cell_type":"code","source":["# Alternatively, if the files has been manually upload to the Files are of the lakehouse we can load the data from there\n","#df = spark.read.format(\"csv\").option(\"header\",\"false\").load(\"Files/pp-complete.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/pp-complete.csv\".\n","#display(df.limit(2))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"36e8637d-1c04-4dce-878b-6cbe28ab99b5"},{"cell_type":"markdown","source":["Count the rows  and show the schema (list of column names and data types)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"174f4d15-a1e0-4ad1-97de-fff283f7325a"},{"cell_type":"code","source":["print(f\"The file has {df.count():,} rows\") \n","df.printSchema()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"237a3d43-8e92-4888-9689-c0f3d2b6056e"},{"cell_type":"markdown","source":[" Before we save the dataframe to a table, configure Spark so that it optimises performance of the tables.  (This is boiler-plate code.)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7c7fdf7e-1bac-4b85-b632-ee413b2b459b"},{"cell_type":"code","source":["spark.conf.set(\"sprk.sql.parquet.vorder.enabled\", \"true\") # Enable Verti-Parquet write\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\") # Enable automatic delta optimized write\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7fd723d9-37dd-4cef-be62-3d4715f28aa8"},{"cell_type":"markdown","source":["If we don't need to clean the file, we can save it now to a table in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"151c80da-5bb3-43a9-ba0d-60208100bb84"},{"cell_type":"code","source":["# table_name = \"PricePaidOriginal_1\"\n","# df.write.mode(\"overwrite\").format(\"delta\").save(f\"Tables/{table_name}\")\n","# print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"16cbc94c-1e40-4d85-a3b9-3d5a804047e6"},{"cell_type":"markdown","source":["We need to clean our file. Firstly We do not need the final 6 columns.  Remove these."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7257d79e-a7e3-4ba1-a642-c7f9cb5cd199"},{"cell_type":"code","source":["df_clean = df.drop(\"_c10\", \"_c11\", \"_c12\", \"_c13\", \"_c14\", \"_c15\")\n","df_clean.limit(2).show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1ce71553-3d32-44b0-b300-e94dd2debaf2"},{"cell_type":"markdown","source":["The original CSV file does not have a column header row.  Rename the first 10 columns to more meaningful names.  \n","See the price paid documentation for more details about the column names."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f465b2cd-abeb-4075-b7af-d57235a561dd"},{"cell_type":"code","source":["df_clean = df_clean\\\n","    .withColumnRenamed(\"_c0\", \"TransactionId\")\\\n","    .withColumnRenamed(\"_c1\", \"Price\")\\\n","    .withColumnRenamed(\"_c2\", \"TransactionDate\")\\\n","    .withColumnRenamed(\"_c3\", \"Postcode\")\\\n","    .withColumnRenamed(\"_c4\", \"PropertyTypeCode\")\\\n","    .withColumnRenamed(\"_c5\", \"IsNewBuild\")\\\n","    .withColumnRenamed(\"_c6\", \"Duration\")\\\n","    .withColumnRenamed(\"_c7\", \"PAON\")\\\n","    .withColumnRenamed(\"_c8\", \"SAON\")\\\n","    .withColumnRenamed(\"_c9\", \"Street\")\n","\n","df_clean.show(2)\n","df_clean.printSchema()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0054745e-120b-43b2-91ac-8002c0ae2aa9"},{"cell_type":"markdown","source":["There are two columns that are not the default string data type.  Change these."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fab17a2a-2f31-4184-89ab-c79dd3db1391"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast('Integer'))\n","df_clean = df_clean.withColumn(\"TransactionDate\",col(\"TransactionDate\").cast('Date'))\n","\n","df_clean.printSchema()\n","df_clean.show(2)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fa54294e-9af5-40e2-8a2b-b2441b2815e7"},{"cell_type":"markdown","source":["Add a new column, TransactionYear, based on the TransactionDate.  We will use this later to plot number of sales by year in Power BI.  \n","This is necessary since it is not possible to create DAX calculated columns in the Power BI web model pane."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7dc12744-b5ac-41f4-a740-7c6478443695"},{"cell_type":"code","source":["from pyspark.sql.functions import year\n","df_clean = df_clean.withColumn(\"TransactionYear\", year(df_clean.TransactionDate))\n","df_clean.printSchema()\n","df_clean.show(2)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"494fe96c-e6b0-4cb3-9da2-51caf7f6646f"},{"cell_type":"markdown","source":["Write the cleaned Spark dataframe to a table in the lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f42000a8-15f7-4056-82aa-4d32fcf2ea36"},{"cell_type":"code","source":["# table_name = \"PricePaidClean_1\"\n","# df.write.mode(\"overwrite\").format(\"delta\").option(\"mergeSchema\", \"true\").save(f\"Tables/{table_name}\")\n","# print(f\"Spark dataframe saved to delta table: {table_name}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4191c1b3-6a59-415c-aabf-f4552830adcb"},{"cell_type":"markdown","source":["We want to create some plots using the pandas and seaborn libraries but 28m rows is too large for efficient pandas operations\n","Firstly, We sample 1 row in 1000 to get a smaller dataset of 28K rows.  Hopefully by doing a random sample, this smaller dataset is representative of the full dataset.\n","We then convert the small sample dataframe from a Spark dataframe to a pandas one."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1207bcfd-c22a-4b97-8cab-0a6cbda3ae2a"},{"cell_type":"code","source":["fraction_to_extract = 0.001\n","df_sample = df_clean.sample(withReplacement=False, fraction=fraction_to_extract)\n","print(f\"The sample file has {df_sample.count():,} rows\")\n","df_sample.show(2)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"89cf3353-fe62-43c0-8aac-ab39ed40fe5b"},{"cell_type":"markdown","source":["Convert the sample dataframe from a Spark to a pandas dataframe (so we can plot with pandas / seaborn)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8aafdf81-c63f-45b9-8a86-6b3ce4a094fb"},{"cell_type":"code","source":["df_pd_sample = df_sample.toPandas()\n","df_pd_sample"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"cef855fe-4d59-4b2d-a366-a015c3506bb6"},{"cell_type":"markdown","source":["Let's create a separate copy to experiment with the Data Wrangler"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"21d75f35-9cc9-470e-aa0d-6d4fadedf2c5"},{"cell_type":"code","source":["df_wrangler = df_pd_sample.copy()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2c46bf05-499d-49a7-851c-1be42db363e6"},{"cell_type":"code","source":["# Code generated by Data Wrangler for pandas DataFrame\n","\n","def clean_data(df_wrangler):\n","    # Clone column 'PropertyTypeCode' as 'PropertyTypeName'\n","    df_wrangler['PropertyTypeName'] = df_wrangler.loc[:, 'PropertyTypeCode']\n","    # Replace all instances of \"T\" with \"Terraced\" in column: 'PropertyTypeName'\n","    df_wrangler.loc[df_wrangler['PropertyTypeName'].str.lower() == \"T\".lower(), 'PropertyTypeName'] = \"Terraced\"\n","    # Replace all instances of \"S\" with \"Semi\" in column: 'PropertyTypeName'\n","    df_wrangler.loc[df_wrangler['PropertyTypeName'].str.lower() == \"S\".lower(), 'PropertyTypeName'] = \"Semi\"\n","    return df_wrangler\n","\n","df_wrangler_clean = clean_data(df_wrangler.copy())\n","df_wrangler_clean.head()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"79f4ca1e-400b-4f40-9843-b43ec0e25432"},{"cell_type":"markdown","source":["Plot the number of transactions by year.  \n","This code was generated by Chat GPT 3.5.  The prompt was _write a python script to plot a chart in seaborn of the number of transactions (rows) by TransactionYear_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"80363b01-4119-4692-9d8c-b8574480d0d5"},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Assuming you have already converted your Spark DataFrame to a Pandas DataFrame\n","# If not, make sure to convert it as shown in the previous response\n","\n","# Plot the number of transactions by TransactionYear\n","plt.figure(figsize=(10, 6))\n","sns.countplot(data=df_pd_sample, x='TransactionYear', palette='viridis')\n","plt.title('Number of Transactions by Year')\n","plt.xlabel('Transaction Year')\n","plt.ylabel('Number of Transactions')\n","plt.xticks(rotation=45)\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b68204c2-4622-47b2-9fee-4c2befcb06b5"},{"cell_type":"markdown","source":["Code written by Chat GPT.  The prompt was \n","_I have a pandas dataframe df_pd_sample.  It has a categorical column PropertyTypeCode.  Count the number of rows grouped by PropertyTypeCode_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bc2219cf-df83-4ca3-a5e9-dd39be7e1f7b"},{"cell_type":"code","source":["# Assuming you have a Pandas DataFrame 'df_pd_sample' with the necessary data\n","# If not, replace 'df_pd_sample' with the actual name of your DataFrame\n","\n","# Group the DataFrame by 'PropertyTypeCode' and count the rows in each group\n","property_type_counts = df_pd_sample.groupby('PropertyTypeCode').size().reset_index(name='Count')\n","\n","# Display the result\n","print(property_type_counts)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"636d0a7b-9034-46be-bce1-a71f60555408"},{"cell_type":"markdown","source":["The property types are:\n","* T = Terraced\n","* F = Flat\n","* S = Semi\n","* D = Detached\n","* O = Other\n","\n","The Other property type includes garages (typically low price) and office buildings (typically high price) so may confuse any predictive model.   \n","Code written by Chat GPT.  The prompt was _Remove the rows where the value of the PropertyTypeCode  is O_"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b53395ab-94c5-4128-9af6-4841a4ab421c"},{"cell_type":"markdown","source":["Let's try Data Wrangler.  Python's answer to the Power BI Query Editor"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"03342e21-f31f-4091-bd17-8e082be7e354"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b9e9e10d-45c3-4a70-bf1b-d8a4ba247d04"},{"cell_type":"markdown","source":["I want meaningingful names, not codes for the property types: Here's my prompt to ChatGPT.  "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"47756591-3bac-40a0-ae2c-f0a8470c374a"},{"cell_type":"code","source":["# Create a mapping dictionary\n","property_type_mapping = {\n","    'O': 'Other',\n","    'D': 'Detached',\n","    'S': 'Semi',\n","    'F': 'Flat',\n","    'T': 'Terraced'\n","}\n","\n","# Use the mapping dictionary to create the new column 'PropertyTypeName'\n","df_pd_sample['PropertyTypeName'] = df_pd_sample['PropertyTypeCode'].map(property_type_mapping)\n","\n","df_pd_sample.head()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4f87eee3-b146-4e65-85ea-96c6c8fa22e2"},{"cell_type":"code","source":["# Assuming you have a Pandas DataFrame 'pandas_df'\n","# If not, make sure to load your data into a Pandas DataFrame\n","\n","# Filter rows where PropertyTypeCode is not equal to 'O'\n","df_pd_sample = df_pd_sample[df_pd_sample['PropertyTypeCode'] != 'O']\n","\n","# Reset the index of the DataFrame\n","df_pd_sample.reset_index(drop=True, inplace=True)\n","\n","#Let's look at the results\n","df_pd_sample.groupby('PropertyTypeCode').size().reset_index(name='Count')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6c6054c3-ba86-472b-b30c-b85e788ed484"},{"cell_type":"markdown","source":["Code originally written by Chat GPT.  The prompt was   \n","_Write Python script to train test and build a regression model.  The dependent variable (label) is the Price column.  use only the columns TransactionYear, PropertyTypeCode and IsNewBuild in the model parameters_\n","\n","I have simplified - removed the IsNewBuild column from the model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"af5f6cca-7bab-4523-b100-6420039bd456"},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error, r2_score\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Assuming you have a Pandas DataFrame 'pandas_df' with the necessary columns\n","# If not, make sure to load your data into a Pandas DataFrame\n","\n","# Select the independent variables and dependent variable\n","#X = df_pd_sample[['TransactionYear', 'PropertyTypeCode', 'IsNewBuild']]\n","X = df_pd_sample[['TransactionYear', 'PropertyTypeCode']]\n","y = df_pd_sample['Price']\n","\n","# Convert categorical columns to numerical using one-hot encoding\n","X = pd.get_dummies(X, columns=['PropertyTypeCode'], drop_first=True)\n","\n","# Split the dataset into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train the linear regression model\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Evaluate the model\n","mse = mean_squared_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"Mean Squared Error: {mse:.2f}\")\n","print(f\"R-squared (R2) Score: {r2:.2f}\")\n","\n","# Plot the actual vs. predicted prices\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(x=y_test, y=y_pred)\n","plt.title('Actual vs. Predicted Prices')\n","plt.xlabel('Actual Price')\n","plt.ylabel('Predicted Price')\n","plt.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f3fbb495-61a8-476f-a67f-e383b207ec98"},{"cell_type":"markdown","source":["Describe the model parameters (with ChatGPT's help)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f5902577-df40-4cdc-9c7d-6ef57bb80933"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"host":{}},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"default_lakehouse":"18c98a71-c402-49a5-a1a8-4551f81b7bd5","known_lakehouses":[{"id":"18c98a71-c402-49a5-a1a8-4551f81b7bd5"}],"default_lakehouse_name":"Property","default_lakehouse_workspace_id":"d372c91d-0b6a-44c5-b4b1-ba07000cf655"}}},"nbformat":4,"nbformat_minor":5}